{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1bcff53-2579-4e02-aae0-0349fa514147",
   "metadata": {},
   "source": [
    "# unigram model without punctuation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fb7baff-74c0-4794-ab94-3268b981cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['this', 'is', 'a', 'simple', 'example', 'to', 'demonstrate', 'a', 'unigram', 'model', 'this', 'example', 'is', 'simple', 'and', 'easy']\n",
      "Word Counts: Counter({'this': 2, 'is': 2, 'a': 2, 'simple': 2, 'example': 2, 'to': 1, 'demonstrate': 1, 'unigram': 1, 'model': 1, 'and': 1, 'easy': 1})\n",
      "Total Words: 16\n",
      "Generated Text: is easy example a simple\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Example text\n",
    "text = \"This is a simple example to demonstrate a unigram model. This example is simple and easy.\"\n",
    "\n",
    "# Step 1: Convert to lowercase and tokenize\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return words\n",
    "\n",
    "words = preprocess(text)\n",
    "print(\"Tokenized Words:\", words)\n",
    "\n",
    "# Step 2: Count word frequencies\n",
    "word_counts = Counter(words)\n",
    "print(\"Word Counts:\", word_counts)\n",
    "\n",
    "# Step 3: Calculate total number of words\n",
    "total_words = sum(word_counts.values())\n",
    "print(\"Total Words:\", total_words)\n",
    "\n",
    "# Step 4: Generate text based on unigram probabilities\n",
    "unique_words = list(word_counts.keys())\n",
    "probabilities = [word_counts[word] / total_words for word in unique_words]\n",
    "\n",
    "def generate_text(num_words):\n",
    "    generated_text = random.choices(unique_words, probabilities, k=num_words)\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# Generate 10 words of text\n",
    "generated_text = generate_text(5)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551700b-f67a-4967-b18a-ca96c5c27547",
   "metadata": {},
   "source": [
    "# unigram model with punctuation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7aeeed5-cfb1-437b-acd0-41049eca0341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['this', 'is', 'a', 'simple', 'example', 'to', 'demonstrate', 'a', 'unigram', 'model', '.', 'this', 'example', 'is', 'simple', 'and', 'easy', '.']\n",
      "Word Counts: Counter({'this': 2, 'is': 2, 'a': 2, 'simple': 2, 'example': 2, '.': 2, 'to': 1, 'demonstrate': 1, 'unigram': 1, 'model': 1, 'and': 1, 'easy': 1})\n",
      "Total Words: 18\n",
      "Generated Text: example to model example unigram to a simple simple example\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Example text\n",
    "text = \"This is a simple example to demonstrate a unigram model. This example is simple and easy.\"\n",
    "\n",
    "# Step 1: Convert to lowercase and tokenize, including punctuation\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b|[.,!?;]', text)\n",
    "    return words\n",
    "\n",
    "# Preprocess the text without removing stop words\n",
    "words = preprocess(text)\n",
    "print(\"Tokenized Words:\", words)\n",
    "\n",
    "# Step 2: Count word frequencies\n",
    "word_counts = Counter(words)\n",
    "print(\"Word Counts:\", word_counts)\n",
    "\n",
    "# Step 3: Calculate total number of words\n",
    "total_words = sum(word_counts.values())\n",
    "print(\"Total Words:\", total_words)\n",
    "\n",
    "# Step 4: Generate text based on unigram probabilities\n",
    "unique_words = list(word_counts.keys())\n",
    "probabilities = [word_counts[word] / total_words for word in unique_words]\n",
    "\n",
    "def generate_text(num_words):\n",
    "    generated_text = random.choices(unique_words, probabilities, k=num_words)\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# Generate 10 words of text\n",
    "generated_text = generate_text(10)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28738b6-46d8-4dee-b198-21393915afce",
   "metadata": {},
   "source": [
    "# simple bigram with punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5af4bc1e-4810-40dc-99f8-0294446432da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['this', 'is', 'a', 'simple', 'example', 'to', 'demonstrate', 'a', 'bigram', 'model', '.', 'this', 'example', 'is', 'simple', 'and', 'easy', '.']\n",
      "Bigram Counts: Counter({('this', 'is'): 1, ('is', 'a'): 1, ('a', 'simple'): 1, ('simple', 'example'): 1, ('example', 'to'): 1, ('to', 'demonstrate'): 1, ('demonstrate', 'a'): 1, ('a', 'bigram'): 1, ('bigram', 'model'): 1, ('model', '.'): 1, ('.', 'this'): 1, ('this', 'example'): 1, ('example', 'is'): 1, ('is', 'simple'): 1, ('simple', 'and'): 1, ('and', 'easy'): 1, ('easy', '.'): 1})\n",
      "Bigram Probabilities: defaultdict(<class 'dict'>, {'this': {'is': 0.5, 'example': 0.5}, 'is': {'a': 0.5, 'simple': 0.5}, 'a': {'simple': 0.5, 'bigram': 0.5}, 'simple': {'example': 0.5, 'and': 0.5}, 'example': {'to': 0.5, 'is': 0.5}, 'to': {'demonstrate': 1.0}, 'demonstrate': {'a': 1.0}, 'bigram': {'model': 1.0}, 'model': {'.': 1.0}, '.': {'this': 0.5}, 'and': {'easy': 1.0}, 'easy': {'.': 1.0}})\n",
      "Generated Text: this example to demonstrate a bigram model. this is\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "# Example text\n",
    "text = \"This is a simple example to demonstrate a bigram model. This example is simple and easy.\"\n",
    "\n",
    "# Step 1: Convert to lowercase and tokenize, preserving punctuation\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    # Split words and punctuation while keeping punctuation adjacent to words\n",
    "    words = re.findall(r'\\b\\w+\\b|[.,!?;]', text)\n",
    "    return words\n",
    "\n",
    "# Preprocess the text\n",
    "words = preprocess(text)\n",
    "print(\"Tokenized Words:\", words)\n",
    "\n",
    "# Step 2: Count bigram frequencies\n",
    "def build_bigrams(words):\n",
    "    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    return bigram_counts\n",
    "\n",
    "bigrams = build_bigrams(words)\n",
    "print(\"Bigram Counts:\", bigrams)\n",
    "\n",
    "# Step 3: Calculate bigram probabilities\n",
    "def calculate_bigram_probabilities(bigrams):\n",
    "    bigram_probabilities = defaultdict(dict)\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    for (w1, w2), count in bigrams.items():\n",
    "        bigram_probabilities[w1][w2] = count / word_counts[w1]\n",
    "        \n",
    "    return bigram_probabilities\n",
    "\n",
    "bigram_probabilities = calculate_bigram_probabilities(bigrams)\n",
    "print(\"Bigram Probabilities:\", bigram_probabilities)\n",
    "\n",
    "# Step 4: Generate text based on bigram probabilities\n",
    "def generate_text(bigram_probabilities, start_word, num_words):\n",
    "    current_word = start_word\n",
    "    generated_text = [current_word]\n",
    "    \n",
    "    for _ in range(num_words - 1):\n",
    "        next_words = list(bigram_probabilities[current_word].keys())\n",
    "        if not next_words:\n",
    "            break\n",
    "        probabilities = list(bigram_probabilities[current_word].values())\n",
    "        next_word = random.choices(next_words, probabilities)[0]\n",
    "        # If next word is punctuation, it should be appended without a space\n",
    "        if next_word in '.,!?;':\n",
    "            generated_text[-1] += next_word\n",
    "        else:\n",
    "            generated_text.append(next_word)\n",
    "        current_word = next_word\n",
    "        \n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# Generate text starting with the word 'this'\n",
    "start_word = 'this'\n",
    "generated_text = generate_text(bigram_probabilities, start_word, 10)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c65d1-47ef-44e6-848d-3b8ffc13fb7f",
   "metadata": {},
   "source": [
    "# class for simple bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7dac26a7-409b-48f5-bf73-5fb70f06ad77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: this is a simple example is simple example to demonstrate a simple example is a simple\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BigramModel:\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    import random\n",
    "    def __init__(self, text):\n",
    "        self.text = text.lower()\n",
    "        self.words = self.preprocess(self.text)\n",
    "        self.bigrams = self.build_bigrams(self.words)\n",
    "        self.bigram_probabilities = self.calculate_bigram_probabilities(self.bigrams, self.words)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Split words and punctuation while keeping punctuation adjacent to words\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return words \n",
    "\n",
    "    def build_bigrams(self, words):\n",
    "        bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "        bigram_counts = Counter(bigrams)\n",
    "        return bigram_counts\n",
    "\n",
    "    def calculate_bigram_probabilities(self, bigrams, words):\n",
    "        bigram_probabilities = {}\n",
    "        word_counts = Counter(words)\n",
    "        \n",
    "        for (w1, w2), count in bigrams.items():\n",
    "            if w1 not in bigram_probabilities:\n",
    "                bigram_probabilities[w1] = {}\n",
    "            bigram_probabilities[w1][w2] = count / word_counts[w1] #conditional probability of w2 given w1\n",
    "            \n",
    "        return bigram_probabilities\n",
    "\n",
    "    def generate_text(self, start_word, num_words):\n",
    "        current_word = start_word\n",
    "        generated_text = [current_word]\n",
    "        \n",
    "        for _ in range(num_words - 1):\n",
    "            if current_word not in self.bigram_probabilities:\n",
    "                break\n",
    "            next_words = list(self.bigram_probabilities[current_word].keys())\n",
    "            probabilities = list(self.bigram_probabilities[current_word].values())\n",
    "            next_word = random.choices(next_words, probabilities)[0]\n",
    "            # If next word is punctuation, it should be appended without a space\n",
    "            if next_word in '.,!?;':\n",
    "                generated_text[-1] += next_word\n",
    "            else:\n",
    "                generated_text.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return ' '.join(generated_text)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a simple example to demonstrate a bigram model. This example is simple and easy.\"\n",
    "bigram_model = BigramModel(text)\n",
    "\n",
    "# Generate text starting with the word 'this'\n",
    "start_word = 'this'\n",
    "generated_text = bigram_model.generate_text(start_word, 16)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02daae5-9940-49bd-9979-64ce09d04824",
   "metadata": {},
   "source": [
    "## bigram with fallback method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1fca89-4d0c-42a7-851a-571a9b4dacae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ab1a7497-213d-4f0d-a02f-f50e24cb79f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: this is\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "class BigramModel:\n",
    "    def __init__(self, text):\n",
    "        self.text = text.lower()\n",
    "        self.words = self.preprocess(self.text)\n",
    "        self.bigrams = self.build_bigrams(self.words)\n",
    "        self.bigram_probabilities = self.calculate_bigram_probabilities(self.bigrams, self.words)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Split words and punctuation while keeping punctuation adjacent to words\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return words \n",
    "\n",
    "    def build_bigrams(self, words):\n",
    "        bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "        bigram_counts = Counter(bigrams)\n",
    "        return bigram_counts\n",
    "\n",
    "    def calculate_bigram_probabilities(self, bigrams, words):\n",
    "        bigram_probabilities = {}\n",
    "        word_counts = Counter(words)\n",
    "        \n",
    "        for (w1, w2), count in bigrams.items():\n",
    "            if w1 not in bigram_probabilities:\n",
    "                bigram_probabilities[w1] = {}\n",
    "            bigram_probabilities[w1][w2] = count / word_counts[w1] #conditional probability of w2 given w1\n",
    "            \n",
    "        return bigram_probabilities\n",
    "\n",
    "    def generate_text(self, start_word, num_words):\n",
    "        current_word = start_word\n",
    "        generated_text = [current_word]\n",
    "        \n",
    "        for _ in range(num_words - 1):\n",
    "            if current_word not in self.bigram_probabilities: #or not self.bigram_probabilities[current_word]:\n",
    "                # Fallback: Choose a new random start word\n",
    "                current_word = random.choice(self.words)\n",
    "                generated_text.append(current_word)\n",
    "                continue\n",
    "            next_words = list(self.bigram_probabilities[current_word].keys())\n",
    "            probabilities = list(self.bigram_probabilities[current_word].values())\n",
    "            next_word = random.choices(next_words, probabilities)[0]\n",
    "            # If next word is punctuation, it should be appended without a space\n",
    "            if next_word in '.,!?;':\n",
    "                generated_text[-1] += next_word\n",
    "            else:\n",
    "                generated_text.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return ' '.join(generated_text)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a simple example to demonstrate a bigram model. This example is simple and easy.\"\n",
    "bigram_model = BigramModel(text)\n",
    "\n",
    "# Generate text starting with the word 'this'\n",
    "start_word = 'this'\n",
    "generated_text = bigram_model.generate_text(start_word, 2)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5153252-b0c2-447c-bd1c-b08f65b8330f",
   "metadata": {},
   "source": [
    "# text generation using neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "325d1f0c-9f79-46aa-af2a-984b90d9a2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0667 - loss: 2.4849\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1333 - loss: 2.4838\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1333 - loss: 2.4826\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1333 - loss: 2.4815\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2000 - loss: 2.4803\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2000 - loss: 2.4791\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2000 - loss: 2.4779\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2000 - loss: 2.4767\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2000 - loss: 2.4755\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2667 - loss: 2.4742\n",
      "Generated Text: this is simple a simple a simple a simple a simple a\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 1: Text Preprocessing\n",
    "text = \"This is a simple example to demonstrate a bigram model. This example is simple and easy.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "# Create sequences for n-gram (bigram in this case)\n",
    "n_gram = 2\n",
    "words = tokenizer.texts_to_sequences([text])[0]\n",
    "sequences = []\n",
    "for i in range(len(words) - n_gram + 1):\n",
    "    sequences.append(words[i:i + n_gram])\n",
    "\n",
    "# Calculate the maximum sequence length\n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Step 2: Build the Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len - 1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the Model\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "model.fit(X, y, epochs=10, verbose=1)\n",
    "\n",
    "# Step 4: Generate Text\n",
    "start_word = 'this is'\n",
    "num_words = 10\n",
    "current_words = start_word.split()\n",
    "generated_text = current_words.copy()\n",
    "\n",
    "for _ in range(num_words):\n",
    "    token_list = tokenizer.texts_to_sequences([current_words])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            output_word = word\n",
    "            break\n",
    "    if not output_word:\n",
    "        break\n",
    "    current_words.append(output_word)\n",
    "    generated_text.append(output_word)\n",
    "    current_words = current_words[1:]\n",
    "\n",
    "print(\"Generated Text:\", ' '.join(generated_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e810d3-527a-4159-bb9b-458da7953fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.2000 - loss: 2.4846\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2667 - loss: 2.4834\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4822\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4809\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2000 - loss: 2.4797\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4785\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2667 - loss: 2.4772\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2667 - loss: 2.4760\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2667 - loss: 2.4747\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4733\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4720\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4706\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4692\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2667 - loss: 2.4677\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4663\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2667 - loss: 2.4647\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2667 - loss: 2.4632\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4615\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4599\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4582\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4564\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4546\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4527\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4507\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2667 - loss: 2.4487\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2667 - loss: 2.4467\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4445\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3333 - loss: 2.4423\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4400\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4377\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4352\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3333 - loss: 2.4327\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3333 - loss: 2.4301\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4274\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4246\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4217\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4187\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4157\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3333 - loss: 2.4125\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3333 - loss: 2.4092\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3333 - loss: 2.4057\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.4022\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.3986\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.3948\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.3909\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3333 - loss: 2.3868\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.3826\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4000 - loss: 2.3783\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4000 - loss: 2.3738\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4000 - loss: 2.3692\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4000 - loss: 2.3644\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4000 - loss: 2.3594\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4000 - loss: 2.3543"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    return tokenizer\n",
    "\n",
    "def create_sequences(text, tokenizer, n_gram=2):\n",
    "    words = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequences = []\n",
    "    for i in range(len(words) - n_gram + 1):\n",
    "        sequences.append(words[i:i + n_gram])\n",
    "    return sequences\n",
    "\n",
    "def build_model(total_words, max_sequence_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=max_sequence_len - 1))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(model, sequences, total_words, max_sequence_len, epochs=100):\n",
    "    sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "    model.fit(X, y, epochs=epochs, verbose=1)\n",
    "\n",
    "def generate_text(model, tokenizer, start_word, num_words, max_sequence_len):\n",
    "    current_words = start_word.split()\n",
    "    generated_text = current_words.copy()\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([current_words])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        if not output_word:\n",
    "            break\n",
    "        current_words.append(output_word)\n",
    "        generated_text.append(output_word)\n",
    "        current_words = current_words[1:]\n",
    "\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a simple example to demonstrate a bigram model. This example is simple and easy.\"\n",
    "\n",
    "# Preprocess the text\n",
    "tokenizer = preprocess_text(text)\n",
    "\n",
    "# Create sequences\n",
    "n_gram = 2  # Change to 3 for trigram, etc.\n",
    "sequences = create_sequences(text, tokenizer, n_gram)\n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Build the model\n",
    "model = build_model(total_words, max_sequence_len)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, sequences, total_words, max_sequence_len, epochs=100)\n",
    "\n",
    "# Generate text starting with the word 'this'\n",
    "start_word = 'this is'\n",
    "generated_text = generate_text(model, tokenizer, start_word, 10, max_sequence_len)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfef182-9607-46c5-92da-05f0f5a67fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class RNNModel:\n",
    "    def __init__(self, text, n_gram=2):\n",
    "        self.text = text.lower()\n",
    "        self.n_gram = n_gram\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts([self.text])\n",
    "        self.total_words = len(self.tokenizer.word_index) + 1\n",
    "        self.sequences = self.create_sequences(self.text)\n",
    "        self.max_sequence_len = max([len(seq) for seq in self.sequences])\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Tokenize the text\n",
    "        return self.tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "    def create_sequences(self, text):\n",
    "        words = self.preprocess(text)\n",
    "        sequences = []\n",
    "        for i in range(len(words) - self.n_gram + 1):\n",
    "            sequences.append(words[i:i + self.n_gram])\n",
    "        return sequences\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.total_words, 10, input_length=self.max_sequence_len - 1))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dense(self.total_words, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs=100):\n",
    "        sequences = np.array(pad_sequences(self.sequences, maxlen=self.max_sequence_len, padding='pre'))\n",
    "        X, y = sequences[:,:-1], sequences[:,-1]\n",
    "        y = tf.keras.utils.to_categorical(y, num_classes=self.total_words)\n",
    "        self.model.fit(X, y, epochs=epochs, verbose=1)\n",
    "\n",
    "    def generate_text(self, start_word, num_words):\n",
    "        current_words = start_word.split()\n",
    "        generated_text = current_words.copy()\n",
    "\n",
    "        for _ in range(num_words):\n",
    "            token_list = self.tokenizer.texts_to_sequences([current_words])[0]\n",
    "            token_list = pad_sequences([token_list], maxlen=self.max_sequence_len - 1, padding='pre')\n",
    "            predicted = self.model.predict(token_list, verbose=0)\n",
    "            predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "            output_word = \"\"\n",
    "            for word, index in self.tokenizer.word_index.items():\n",
    "                if index == predicted_word_index:\n",
    "                    output_word = word\n",
    "                    break\n",
    "            if not output_word:\n",
    "                break\n",
    "            current_words.append(output_word)\n",
    "            generated_text.append(output_word)\n",
    "            current_words = current_words[1:]\n",
    "\n",
    "        return ' '.join(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12ebb64-a968-41cd-a130-e9b0b896b416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.1333 - loss: 2.4849\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1333 - loss: 2.4838\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1333 - loss: 2.4826\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1333 - loss: 2.4815\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1333 - loss: 2.4803\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2000 - loss: 2.4792\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3333 - loss: 2.4780\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3333 - loss: 2.4768\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3333 - loss: 2.4756\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3333 - loss: 2.4743\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3333 - loss: 2.4731\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3333 - loss: 2.4718\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3333 - loss: 2.4705\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3333 - loss: 2.4691\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3333 - loss: 2.4677\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.3333 - loss: 2.4663\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2667 - loss: 2.4648\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2667 - loss: 2.4633\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2667 - loss: 2.4618\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2667 - loss: 2.4602\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2667 - loss: 2.4585\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2667 - loss: 2.4568\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2667 - loss: 2.4550\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2667 - loss: 2.4532\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2667 - loss: 2.4513\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2667 - loss: 2.4494\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2667 - loss: 2.4474\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2667 - loss: 2.4453\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2667 - loss: 2.4431\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2667 - loss: 2.4409\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2667 - loss: 2.4386\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2667 - loss: 2.4362\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2667 - loss: 2.4338\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2667 - loss: 2.4312\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.2667 - loss: 2.4286\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2667 - loss: 2.4259\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.2667 - loss: 2.4231\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2667 - loss: 2.4202\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2667 - loss: 2.4172\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2667 - loss: 2.4140\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2667 - loss: 2.4108\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2667 - loss: 2.4075\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2667 - loss: 2.4041\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3333 - loss: 2.4005\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3333 - loss: 2.3968\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3333 - loss: 2.3930\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3333 - loss: 2.3891\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3333 - loss: 2.3850\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3333 - loss: 2.3809\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3333 - loss: 2.3765\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3333 - loss: 2.3721\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3333 - loss: 2.3674\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3333 - loss: 2.3627\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3333 - loss: 2.3578\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3333 - loss: 2.3527\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3333 - loss: 2.3475\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3333 - loss: 2.3421\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3333 - loss: 2.3365\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3333 - loss: 2.3307\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3333 - loss: 2.3248\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3333 - loss: 2.3187\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3333 - loss: 2.3124\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3333 - loss: 2.3060\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3333 - loss: 2.2993\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3333 - loss: 2.2924\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3333 - loss: 2.2854\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3333 - loss: 2.2781\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4000 - loss: 2.2706\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4000 - loss: 2.2629\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.4000 - loss: 2.2550\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4000 - loss: 2.2469\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4000 - loss: 2.2385\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4000 - loss: 2.2300\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4000 - loss: 2.2212\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4000 - loss: 2.2121\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.4000 - loss: 2.2028\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4000 - loss: 2.1933\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4000 - loss: 2.1836\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4000 - loss: 2.1736\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.4000 - loss: 2.1633\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4000 - loss: 2.1529\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4000 - loss: 2.1422\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4000 - loss: 2.1312\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4000 - loss: 2.1200\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4000 - loss: 2.1085\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4000 - loss: 2.0969\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4000 - loss: 2.0849\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.4000 - loss: 2.0728\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.4000 - loss: 2.0604\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4000 - loss: 2.0478\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4000 - loss: 2.0350\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4000 - loss: 2.0219\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4000 - loss: 2.0087\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.4000 - loss: 1.9952\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4000 - loss: 1.9816\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4000 - loss: 1.9678\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4000 - loss: 1.9538\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4000 - loss: 1.9396\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.4000 - loss: 1.9253\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4000 - loss: 1.9108\n",
      "Generated Text: this is a simple example is a simple example is a simple\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"This is a simple example to demonstrate a bigram model. This example is simple and easy.\"\n",
    "rnn_model = RNNModel(text, n_gram=2)\n",
    "rnn_model.train(epochs=100)\n",
    "\n",
    "# Generate text starting with the word 'this'\n",
    "start_word = 'this is'\n",
    "generated_text = rnn_model.generate_text(start_word, 10)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35259e7a-00ee-47c9-8426-13c76b977088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
